{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSazE5Asexzd"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import os\n",
        "\n",
        "from images import load_original_images, load_generated_images, generate_and_save_images\n",
        "from unet import load_unet, save_unet, train_unet, save_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWhC4RIeRNrj"
      },
      "outputs": [],
      "source": [
        "# SD-Turbo text-to-image pipeline\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipeline.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BETGbqPVZRyL"
      },
      "outputs": [],
      "source": [
        "def transform(examples: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Formatting transform for a text-to-image dataset.\n",
        "\n",
        "    Args:\n",
        "        examples (`dict`):\n",
        "            Batch of examples.\n",
        "\n",
        "    Returns:\n",
        "        `dict`: Batch of transformed examples.\n",
        "    \"\"\"\n",
        "\n",
        "    # Function to preprocess images before VAE encoding\n",
        "    preprocess = v2.Compose([\n",
        "        v2.Resize((512, 512)),\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize([0.5], [0.5]),\n",
        "    ])\n",
        "\n",
        "    # Preprocess images and tokenize text\n",
        "    pixel_values = [preprocess(image) for image in examples[\"image\"]]\n",
        "    input_ids = pipeline.tokenizer(examples[\"text\"], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSO2e6OxqSjQ"
      },
      "outputs": [],
      "source": [
        "# Prompts to use as inputs\n",
        "prompts = {\n",
        "    \"main\": \"a photo capturing student life on campus at the University of Toronto\",\n",
        "    \"similar\": \"a photo capturing student life on campus at the University of Waterloo\",\n",
        "    \"different\": \"a wide shot of Santa Monica Beach\",\n",
        "}\n",
        "\n",
        "# Number of images to generated for each prompt per generation\n",
        "num_images = {\"main\": 75, \"similar\": 25, \"different\": 25}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VAbyZuSnZ5RY"
      },
      "outputs": [],
      "source": [
        "# Generation loop\n",
        "for generation in range(20):\n",
        "    print(f\"Generation: {generation}\")\n",
        "\n",
        "    # Load generated dataset and UNet for subsequent generations\n",
        "    if generation > 0:\n",
        "\n",
        "        # Load generated dataset from previous generation\n",
        "        print(\" - Loading Dataset\")\n",
        "        dataset = load_generated_images(prompts, \"main\", generation - 1)\n",
        "        dataset.set_transform(transform)\n",
        "\n",
        "        # Load UNet from previous generation\n",
        "        print(\" - Loading UNet\")\n",
        "        load_unet(pipeline, generation - 1)\n",
        "\n",
        "    # Load original dataset for first generation\n",
        "    else:\n",
        "\n",
        "        # Load original dataset\n",
        "        print(\" - Loading Dataset\")\n",
        "        dataset = load_original_images(prompts, \"main\")\n",
        "        dataset.set_transform(transform)\n",
        "\n",
        "    # Training loop\n",
        "    print(\" - Training UNet\")\n",
        "    train_loss = train_unet(pipeline, dataset)\n",
        "\n",
        "    # Save training loss\n",
        "    print(\" - Saving Train Loss\")\n",
        "    save_train_loss(train_loss, generation)\n",
        "\n",
        "    # Save UNet state\n",
        "    print(\" - Saving UNet\")\n",
        "    save_unet(pipeline, generation)\n",
        "\n",
        "    # Generate and save images\n",
        "    print(\" - Generating and Saving Images\")\n",
        "    for prompt_name in prompts:\n",
        "        generate_and_save_images(pipeline, prompts, prompt_name, generation, num_images[prompt_name])\n",
        "\n",
        "    # Compress all data from this generation\n",
        "    print(\" - Compressing\")\n",
        "    os.system(f\"tar -C data -czf data/gen_{generation}.tar.gz gen_{generation}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
